{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLimputer - Performance Evaluation\n",
    "\n",
    "This notebook demonstrates how to:\n",
    "1. Compare multiple imputation strategies\n",
    "2. Use cross-validation for robust evaluation\n",
    "3. Identify the best performing strategy\n",
    "4. Evaluate on holdout test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, GradientBoostingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from mlimputer import MLimputer\n",
    "from mlimputer.evaluation.evaluator import Evaluator\n",
    "from mlimputer.schemas.parameters import imputer_parameters, update_model_config\n",
    "from mlimputer.data.data_generator import ImputationDatasetGenerator\n",
    "from mlimputer.utils.serialization import ModelSerializer\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"MLIMPUTER - PERFORMANCE EVALUATION\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Binary Classification Dataset\n",
    "\n",
    "Create a dataset with 2000 samples and 15% missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = ImputationDatasetGenerator(random_state=42)\n",
    "\n",
    "TASK = \"binary_classification\"\n",
    "X, y = generator.quick_binary(n_samples=2000, missing_rate=0.15)\n",
    "\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Dataset: {X.shape}\")\n",
    "print(f\"Missing: {X.isnull().sum().sum()} values\")\n",
    "print(f\"Target distribution:\\n{y.value_counts()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Predictive Models\n",
    "\n",
    "We'll evaluate imputation strategies using multiple classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictive_models = [\n",
    "    RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "    ExtraTreesClassifier(n_estimators=50, random_state=42),\n",
    "    GradientBoostingClassifier(n_estimators=50, learning_rate=0.1, random_state=42),\n",
    "    DecisionTreeClassifier(max_depth=10, random_state=42),\n",
    "]\n",
    "\n",
    "primary_metric = \"F1\"\n",
    "\n",
    "print(f\"Predictive models: {[m.__class__.__name__ for m in predictive_models]}\")\n",
    "print(f\"Primary metric: {primary_metric}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([X, y], axis=1)\n",
    "train_size = int(0.8 * len(data))\n",
    "train = data.iloc[:train_size].reset_index(drop=True)\n",
    "test = data.iloc[train_size:].reset_index(drop=True)\n",
    "\n",
    "print(f\"Training set: {train.shape}\")\n",
    "print(f\"Test set: {test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure Imputation Strategies\n",
    "\n",
    "We'll compare 4 different strategies:\n",
    "- Random Forest\n",
    "- Extra Trees  \n",
    "- Gradient Boosting\n",
    "- KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = imputer_parameters()\n",
    "\n",
    "# Customize parameters\n",
    "params[\"RandomForest\"] = update_model_config(\n",
    "    \"RandomForest\",\n",
    "    {\"n_estimators\": 50, \"max_depth\": 10}\n",
    ")\n",
    "params[\"ExtraTrees\"][\"n_estimators\"] = 50\n",
    "params[\"GBR\"][\"learning_rate\"] = 0.05\n",
    "params[\"KNN\"][\"n_neighbors\"] = 7\n",
    "\n",
    "strategies = [\"RandomForest\", \"ExtraTrees\", \"GBR\", \"KNN\"]\n",
    "print(f\"Strategies to evaluate: {strategies}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Cross-Validation Evaluation\n",
    "\n",
    "Use 3-fold cross-validation to compare strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = Evaluator(\n",
    "    imputation_models=strategies,\n",
    "    train=train,\n",
    "    target=\"target\",\n",
    "    n_splits=3,\n",
    "    hparameters=params,\n",
    "    problem_type=TASK\n",
    ")\n",
    "\n",
    "cv_results = evaluator.evaluate_imputation_models(models=predictive_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Identify Best Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_imputer = evaluator.get_best_imputer()\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"✓ Best imputation strategy: {best_imputer}\")\n",
    "print(f\"{'='*60}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Top Results\n",
    "\n",
    "Show top 5 model-strategy combinations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate = cv_results[cv_results[\"Fold\"] == \"Aggregate\"]\n",
    "metric_col = f\"{primary_metric} Mean\"\n",
    "top_results = aggregate.nlargest(5, metric_col)\n",
    "\n",
    "print(f\"\\nTop 5 combinations by {primary_metric}:\")\n",
    "top_results[[\"Model\", \"Imputer Model\", metric_col]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate on Test Set\n",
    "\n",
    "Test the best strategy on holdout data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = evaluator.evaluate_test_set(\n",
    "    test=test,\n",
    "    imput_model=best_imputer,\n",
    "    models=predictive_models\n",
    ")\n",
    "\n",
    "print(\"\\nTest Set Performance:\")\n",
    "test_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Best Model\n",
    "\n",
    "Save the best imputation strategy for production use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit on full training set\n",
    "best_imputer_model = MLimputer(\n",
    "    imput_model=best_imputer,\n",
    "    imputer_configs=params\n",
    ")\n",
    "best_imputer_model.fit(X=train.drop(columns=['target']))\n",
    "\n",
    "# Save configuration\n",
    "best_config = {\n",
    "    \"strategy\": best_imputer,\n",
    "    \"parameters\": params.get(best_imputer, {}),\n",
    "    \"task\": TASK,\n",
    "    \"primary_metric\": primary_metric\n",
    "}\n",
    "\n",
    "ModelSerializer.save(\n",
    "    obj=best_imputer_model,\n",
    "    filepath=\"best_imputer.joblib\",\n",
    "    format=\"joblib\",\n",
    "    metadata=best_config\n",
    ")\n",
    "\n",
    "print(\"✓ Model saved: best_imputer.joblib\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = evaluator.get_summary_report()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATION SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nDataset: {summary['dataset_shape'][0]} samples, {summary['dataset_shape'][1]} features\")\n",
    "print(f\"Task: {TASK}\")\n",
    "print(f\"Best imputer: {best_imputer}\")\n",
    "print(f\"Primary metric: {primary_metric}\")\n",
    "print(f\"Strategies tested: {len(strategies)}\")\n",
    "print(f\"Models evaluated: {len(predictive_models)}\")\n",
    "print(\"\\n✓ Evaluation completed!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}